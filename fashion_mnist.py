import mlflow.pytorch
from ray import serve
import os
import logging

from io import BytesIO
from PIL import Image
from starlette.requests import Request
from typing import Dict

import torch
from torchvision import transforms


@serve.deployment
class ImageModel:
    def __init__(self):
        self.model = mlflow.pytorch.load_model(os.environ["MODEL_PATH"])
        self.preprocessor = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ]
        )
        self.logger = logging.getLogger("ray.serve")

    async def __call__(self, starlette_request: Request) -> Dict:
        image_payload_bytes = await starlette_request.body()
        pil_image = Image.open(BytesIO(image_payload_bytes))
        self.logger.info("[1/3] Parsed image data: {}".format(pil_image))

        pil_images = [pil_image]  # Our current batch size is one
        input_tensor = torch.cat(
            [self.preprocessor(i).unsqueeze(0) for i in pil_images]
        )
        self.logger.info("[2/3] Images transformed, tensor shape {}".format(input_tensor.shape))

        with torch.no_grad():
            output_tensor = self.model(input_tensor)
        self.logger.info("[3/3] Inference done!")
        return {"class_index": int(torch.argmax(output_tensor[0]))}


deploy = ImageModel.bind()
